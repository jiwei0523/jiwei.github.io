
<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="IE=7.0000" http-equiv="X-UA-Compatible">
<title>Wei Ji's Homepage</title>
<meta name="description" content="Wei Ji, Research Fellow in National University of Singapore. Computer Vision, Video Understanding, Vision and Language).">
<meta name="keywords" content="Wei Ji, National University of Singapore, multi media, video understanding, deep learning, machine learning">

<link rel="stylesheet" type="text/css" href="./files/weiji.css">
<!-- <script type="text/javascript" async="" src="./files/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39532305-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script> -->

<style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style></head>


<body>

<div id="content">

<div id="news">
    <h2>News</h2><br>
    <font size="3px">


    <b> Nov 2023</b><br>
    <span class="easylink">
      I will serve as an Area Chair for ACM Multimedia 2024.
    </span><br><br>
    </font>
	    
    <b> Oct 2023</b><br>
    <span class="easylink">
      One full paper is accepted by EMNLP, about video reasoning. One full paper is accepted by TPAMI, about video question answering.
    </span><br><br>
    </font>


	
    <b> Sep 2023</b><br>
    <span class="easylink">
      One full paper is accepted by NeurIPS, about visual prompt generator. 
    </span><br><br>
    </font>
	    
    <b> July 2023</b><br>
    <span class="easylink">
      Four full papers are accepted by ACM MM, about Video Moment Retrieval, Scene Graph Generation, Visual Instance Retrieval, and Multi-modal Recommendation. 
    </span><br><br>
    </font>

	    
    <b> July 2023</b><br>
    <span class="easylink">
      Two full papers are accepted by ICCV, about Scene Graph Generation and Prompt Learning. 
    </span><br><br>
    </font>
	
    <b> May 2023</b><br>
    <span class="easylink">
      Two full papers are accepted by ACL, about Image Captioning and Visual Spatial Description. One paper is accepted by ACL Findings, about video-based fake news detection.
    </span><br><br>
    </font>
	
	
    <b> April 2023</b><br>
    <span class="easylink">
      We are hosting the 1st Deep Multimodal Learning for Information Retrieval (ACM MM2023, Ottawa). Please visit <a href="https://vru-next.github.io/MMIR23/" target="_blank">here</a> for details.
    </span><br><br>
    </font>
	    
    <b> February 2023</b><br>
    <span class="easylink">
     Two full papers are accepted by CVPR, about video moment retrieval and spatial-temporal video grounding.
    </span><br><br>
    </font>
	
	
    <b> November 2022</b><br>
    <span class="easylink">
     Two full papers are accepted by AAAI, about video-audio domain generalization and video-based fake news detection.
    </span><br><br>
    </font>
	
    <b> October 2022</b><br>
    <span class="easylink">
     One full paper is accepted by WSDM, about microvideo-Product retrieval.
    </span><br><br>
    </font>
	
    <b> October 2022</b><br>
    <span class="easylink">
     Two full papers are accepted by EMNLP, about video question answering and pretrained vision-language model.
    </span><br><br>
    </font>	
	    
    <b> July 2022</b><br>
    <span class="easylink">
     One full paper is accepted by ECCV, about scene graph generation.
    </span><br><br>
    </font>
	
	
    <b> May 2022</b><br>
    <span class="easylink">
     One full paper is accepted by TIP, about image super-resolution.
    </span><br><br>
    </font>	 
	
    <b> April 2022</b><br>
    <span class="easylink">
     One full paper is accepted by SIGIR'22, about conversational search.
    </span><br><br>
    </font>	    
	    
    <b> March 2022</b><br>
    <span class="easylink">
     One full paper is accepted by CVPR'22, about video question answering.
    </span><br><br>
    </font>
	
    <b> Dec 2021</b><br>
    <span class="easylink">
     Three full papers are accepted by AAAI'22, about video question answering, grounded situation recognition, and image quality assessment.
    </span><br><br>
    </font>
	    
	    
    <b> August 2021</b><br>
    <span class="easylink">
     One full paper is accepted by ACM MM'21, about video relation detection.
    </span><br><br>
    </font>
	    
    <b> July 2021</b><br>
    <span class="easylink">
     One full paper is accepted by SIGIR'21, about natural language video localization.
    </span><br><br>
    </font>
	
    <b> May 2021</b><br>
    <span class="easylink">
      We are hosting the 3rd Video Relation Understanding Grand Challenge (ACM MM2021, Chengdu). Please visit <a href="https://videorelation.nextcenter.org" target="_blank">here</a> for details.
    </span><br><br>
    </font>
	
    <b> December 2020</b><br>
    <span class="easylink">
      One full paper is accepted by AAAI'21, about natural language video localization.
    </span><br><br>
    </font>

    <b> August 2020</b><br>
    <span class="easylink">
      I have successfully defended my thesis and got the PhD degree! My thesis title is "Research on pixel-level semantic understanding based on deep learning".
    </span><br><br>
    </font>

</div>

<div id="left">
<table style="background-color:white;">
<tbody><tr nosave="">
<td valign="CENTER">
<img src="./images/weiji.JPG" height="250" align="left">
</td>

<td valign="CENTER" width="2%">
</td>

<td valign="CENTER" halign="LEFT">
<font size="+0">
<b><font size="+2">Wei Ji&nbsp;</font></b>
<!--<p style="margin-left:0px;">
<img src="./images/name.png", height="60">
</p>-->
<p style="margin-left:0px;">
<b>Research Fellow</b>
</p><p style="margin-left:0px;">
<a href="http://www.nextcenter.org/", target="_blank">NExT++</a><br/>
<a href="https://www.comp.nus.edu.sg/", target="_blank">School of Computing</a><br/>
<a href="http://www.nus.edu.sg", target="_blank">National University of Singapore</a><br/>
</p><p style="margin-left:0px;">
Computing 1, Computing Drive, Singapore 117417<br>
</p><p style="margin-left:0px;">
Email: weiji0523 AT gmail.com</a><br>
</p><p style="margin-left:0px;">
Email: jiwei AT nus.edu.sg</a><br>
&bull; <a href="https://scholar.google.com/citations?user=69OFB-AAAAAJ&hl=en">Google Scholar</a> <be>
&bull; <a href="https://openreview.net/profile?id=~Wei_Ji1">OpenReview</a> <br>
</p></font><p><font size="+0">
</font>
</p></td>
</tr>
</tbody></table>

<div style="margin-top:20px;">
    I am a research fellow in the School of Computing, National University of Singapore, working with <a href="https://www.chuatatseng.com/">Prof. Tat-seng Chua</a> and <a href="https://www.comp.nus.edu.sg/cs/people/rogerz/">Prof. Roger Zimmermann</a>. I have published several papers in top conferences such as SIGIR, CVPR, ECCV, ACM MM, EMNLP 
    and journals including TPAMI, TIP and TCYB. My research interests include but are not limited to: Cross-modal Retrieval, Multi-modal Learning, and Multi-modal Generation.
    Moreover, I have served as the PC member for top-tier conferences/journals, including SIGIR, CVPR, ICCV, ECCV, AAAI, ACM MM, EMNLP, IJCV, TIP, TMM, etc.
    
</div>

<div style="margin-top:20px;">
    I am actively seeking highly motivated interns who share my research interests. Kindly reach out to me at weiji0523@gmail.com with your resume!
</div>





<!-- =======================================================================!-->



<div id="papers">
<h2 style="CLEAR: both">Publications</h2>
</br>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://ieeexplore-ieee-org.libproxy1.nus.edu.sg/abstract/document/10214041/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Transformer-empowered invariant grounding for video question answering</span> 
      <br>Yicong Li, Xiang Wang, Junbin Xiao, <b>Wei Ji </b>, Tat-Seng Chua
    <br>TPAMI 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">ART: rule bAsed futuRe-inference deducTion</span> 
      <br>Mengze Li, Tianqi Zhao, Bai Jionghao, Baoyi He, Jiaxu Miao, Wenqiao Zhang, Zheqi Lv, Zhou Zhao, Shengyu Zhang,  <b>Wei Ji </b>, Fei Wu
    <br>EMNLP 2023 (Main)&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2305.01278.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Transfer visual prompt generator across llms</span> 
      <br>Ao Zhang, Hao Fei, Yuan Yao, <b>Wei Ji </b>, Li Li, Zhiyuan Liu, Tat-Seng Chua
    <br>NeurIPS 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Learning Style-Invariant Robust Representation for Generalizable Visual Instance Retrieval</span> 
      <br>Tianyu Chang, Xun Yang, Xin Luo, <b>Wei Ji </b>, Meng Wang
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Biased-Predicate Annotation Identification via Unbiased Visual Predicate Representation</span> 
      <br>Li Li, Chenwei Wang, You Qin, <b>Wei Ji </b>, Renjie Liang
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Partial Annotation-based Video Moment Retrieval via Iterative Learning</span> 
      <br><b>Wei Ji </b>, Renjie Liang, Lizi Liao, Hao Fei, Fuli Feng
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2308.04067.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Online Distillation-enhanced Multi-modal Transformer for Sequential Recommendation</span> 
      <br><b>Wei Ji </b>, Xiangyan Liu, An Zhang, Yinwei Wei, Yongxin Ni, Xiang Wang
    <br>ACM MM 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2303.13233.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World.</span> 
      <br>Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, <b>Wei Ji </b>, Yueting Zhuang
    <br>ICCV 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>	

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2303.06571.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models.</span> 
      <br>Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, <b>Wei Ji </b>, Qi Tian, Tat-Seng Chua, Yueting Zhuang
    <br>ICCV 2023&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

	
<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2306.05241.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating.</span> 
      <br>Peng Qi, Yuyang Zhao, Yufeng Shen, <b>Wei Ji </b>, Juan Cao, Tat-Seng Chua.
    <br>ACL 2023 Findings&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>
	

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2305.11768.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Generating Visual Spatial Description via Holistic 3D Scene Understanding.</span> 
      <br>Yu Zhao, Hao Fei,<b>Wei Ji </b>, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua.
    <br>ACL 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2305.12260.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment.</span> 
      <br>Shengqiong Wu, Hao Fei, <b>Wei Ji</b>, Tat-Seng Chua.
    <br>ACL 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>



<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">WINNER: Weakly-supervised hIerarchical decompositioN and aligNment for spatio-tEmporal video gRounding.</span> 
      <br>Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao,<b>Wei Ji</b>, Zhou Zhao, Shengyu Zhang, Fei Wu.
    <br>CVPR 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Are_Binary_Annotations_Sufficient_Video_Moment_Retrieval_via_Hierarchical_Uncertainty-Based_CVPR_2023_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Are Binary Annotations Sufficient? Video Moment Retrieval via Hierarchical Uncertainty-based Active Learning.</span> 
      <br><b>Wei Ji</b>, Renjie Liang, Zhedong Zheng, Wenqiao Zhang, Shengyu Zhang, Juncheng, Li and Mengze Li, Tat-Seng Chua.
    <br>CVPR 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://jiwei0523.github.io/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video-Audio Domain Generalization via Confounder Disentanglement.</span> 
      <br>Shengyu Zhang, Xusheng Feng, Wenyan Fan, Wenjing Fang, Fuli Feng, <b>Wei Ji</b>, Li Shuo, Li Wang, Shanshan Zhao, Zhou Zhao, Tat-Seng Chua, Fei Wu.
    <br>AAAI 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2211.10973.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">FakeSV: A Multimodal Benchmark with Rich Social Context for Fake News Detection on Short Video Platforms</span> 
      <br>Peng Qi, Yuyan Bu, Juan Cao, <b>Wei Ji</b>, Ruihao Shui, Junbin Xiao, Danding Wang, Tat-Seng Chua
    <br>AAAI 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2212.11471.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Multi-queue Momentum Contrast for Microvideo-Product Retrieval</span> 
      <br>Yali Du, Yinwei Wei, <b>Wei Ji</b>, Fan Liu, Xin Luo, Liqiang Nie
    <br>WSDM 2023 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2205.11169" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models</span> 
      <br>Yuan Yao, Qianyu Chen, Ao Zhang, <b>Wei Ji</b>, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun
    <br>EMNLP 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2203.01225" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video Question Answering: Datasets, Algorithms and Challenges</span> 
      <br>Yaoyao Zhong, Junbin Xiao, <b>Wei Ji</b>, Yicong Li, Weihong Deng, Tat-Seng Chua
    <br>EMNLP 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="doi.acm.org?doi=3477495.3532063" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Structured and Natural Responses Co-generation for Conversational Search</span> 
      <br>Chenchen Ye, Lizi Liao, Fuli Feng, <b>Wei Ji</b>, Tat-Seng Chua
    <br>SIGIR 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/abs/2203.11654.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Fine-Grained Scene Graph Generation with Data Transfer</span> 
      <br>Ao Zhang, Yuan Yao, Qianyu Chen, <b>Wei Ji</b>, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua
    <br>ECCV 2022 (oral)&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2104.03926.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations</span> 
      <br>Guanghao Yin, Wei Wang, Zehuan Yuan, <b>Wei Ji</b>, Dongdong Yu, Shouqian Sun, Tat-Seng Chua, Changhu Wang
    <br>TIP 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Invariant grounding for video question answering</span> 
      <br>Yicong Li, Xiang Wang, Junbin Xiao, <b>Wei Ji</b>, Tat-Seng Chua
    <br>CVPR 2022 (oral, Best Paper Finalist) &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2202.13123" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Content-Variant Reference Image Quality Assessment via Knowledge Distillation</span> 
      <br>Guanghao Yin, Wei Wang, Zehuan Yuan, Chuchu Han, <b>Wei Ji</b>, Shouqian Sun, Changhu Wang
    <br>AAAI 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2112.05375" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Rethinking the Two-Stage Framework for Grounded Situation Recognition</span> 
      <br>Meng Wei, Long Chen, <b>Wei Ji</b>, Xiaoyu Yue, Tat-Seng Chua
    <br>AAAI 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2112.06197" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video as Conditional Graph Hierarchy for Multi-Granular Question Answering</span> 
      <br>Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, <b>Wei Ji</b>, Tat-Seng Chua
    <br>AAAI 2022 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://arxiv.org/pdf/2105.12694?ref=https://githubhelp.com" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Deep Learning for Weakly-Supervised Object Detection and Localization: A Survey</span> 
      <br>Feifei Shao, Long Chen, Jian Shao, <b>Wei Ji</b>, Shaoning Xiao, Lu Ye, Yueting Zhuang, Jun Xiao
    <br>Neurocomputing &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3479232" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">VidVRD 2021: The Third Grand Challenge on Video Relation Detection</span> 
      <br><b>Wei Ji</b>, Yicong Li, Meng Wei, Xindi Shang, Junbin Xiao, Tongwei Ren, Tat-Seng Chua
    <br>ACM MM 2021 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3404835.3462823" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Deconfounded Video Moment Retrieval with Causal Intervention</span> 
      <br>Xun Yang, Fuli Feng, <b>Wei Ji</b>, Meng Wang, Tat-Seng Chua
    <br>SIGIR 2021 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475263" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Video Visual Relation Detection via Iterative Inference</span> 
      <br>Xindi Shang, Yicong Li, Junbin Xiao, <b>Wei Ji</b>, Tat-Seng Chua
    <br>ACM MM 2021 &nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://www.aaai.org/AAAI21Papers/AAAI-6267.XiaoS.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Boundary Proposal Network for Two-Stage Natural Language Video Localization</span> 
      <br>Shaoning Xiao, Long Chen, Songyang Zhang, <b>Wei Ji</b>, Jian Shao, Lu Ye, Jun Xiao
    <br>AAAI 2021&nbsp;&nbsp; 
  </td> 
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://ieeexplore.ieee.org/abstract/document/9126215" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Context-Aware Graph Label Propagation Network for Saliency Detection</span>
        <br><b>Wei Ji</b>, Xi Li, Lina Wei, Fei Wu, Yueting Zhuang
    <br>IEEE Transactions on Image Processing (TIP 2020) &nbsp;&nbsp;
  </td>
  </tr>
 </tbody>
</table>



<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://ieeexplore.ieee.org/abstract/document/8943115" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Human-centric clothing segmentation via deformable semantic locality-preserving network</span>
        <br><b>Wei Ji</b>, Xi Li, Fei Wu, Zhijie Pan, Yueting Zhuang
    <br>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2019) &nbsp;
  </td>
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://ieeexplore.ieee.org/abstract/document/8563043" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Context-aware deep spatiotemporal network for hand pose estimation from depth images</span>
        <br>Yiming Wu, <b>Wei Ji</b>, Xi Li, Gang Wang, Jianwei Yin, Fei Wu
    <br>IEEE Transactions on Cybernetics (TCYB 2018) &nbsp;
  </td>
  </tr>
 </tbody>
</table>


<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://ieeexplore.ieee.org/abstract/document/8322285" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Multi-task structure-aware context modeling for robust keypoint-based object tracking</span>
        <br>Xi Li, Liming Zhao, <b>Wei Ji</b>, Yiming Wu, Fei Wu, Ming-Hsuan Yang, Dacheng Tao, Ian Reid
    <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2018) &nbsp;
  </td>
  </tr>
 </tbody>
</table>

<table>
  <tbody>
  <tr>
    <td class="left"><a href="https://www.researchgate.net/profile/Jiabao-Cui/publication/326205821_Semantic_Locality-Aware_Deformable_Network_for_Clothing_Segmentation/links/5cab5c3e299bf118c4bad81a/Semantic-Locality-Aware-Deformable-Network-for-Clothing-Segmentation.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Semantic Locality-Aware Deformable Network for Clothing Segmentation</span>
        <br><b>Wei Ji</b>, Xi Li, Yueting Zhuang, Omar El Farouk Bourahla, Yixin Ji, Shihao Li, Jiabao Cui
    <br>International Joint Conference on Artificial Intelligence (IJCAI 2018) &nbsp;
  </td>
  </tr>
 </tbody>
</table>





<h2 style="CLEAR: both;">Professional Services</h2>
<table><tbody><tr><td>
<!-- 	Local Chair of <span class="title">CCIS 2019</span> (IEEE International Conference on Cloud Computing and Intelligence Systems) <br> -->
    Program Committee Member of <span class="title">ACL (2023) </span> <br>  
    Program Committee Member of <span class="title">NeurIPS (2023) </span> <br>  
    Program Committee Member of <span class="title">WWW (2023) </span> <br>  
    Program Committee Member of <span class="title">SIGIR (2023) </span> <br> 
    Program Committee Member of <span class="title">CVPR (2022,2023,2024) </span> <br>
    Program Committee Member of <span class="title">ICCV (2023) </span> <br>
    Program Committee Member of <span class="title">ECCV (2022) </span> <br>
    Program Committee Member of <span class="title">AAAI (2021,2022,2023,2024) </span> <br>
    Program Committee Member of <span class="title">ACM MM (2021,2022,2023) </span> <br>
    Program Committee Member of <span class="title">EMNLP (2022) </span> <br>
    Program Committee Member of <span class="title">ACM Multimedia Asia (2021) </span> <br>
    Invited Reviewer for <span class="title">International Journal of Computer Vision (IJCV)</span> <br>
    Invited Reviewer for <span class="title">IEEE Transactions on Image Processing (TIP)</span> <br>
    Invited Reviewer for <span class="title">IEEE Transactions on Multimedia (TMM)</span> <br>
    Invited Reviewer for <span class="title">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</span> <br>
</td></tr></tbody></table>



<h2 style="CLEAR: both;">Honors</h2>

<table><tbody><tr><td>
  <span class="title">Outstanding Graduate, Zhejiang University&nbsp;&nbsp; 2020</span> &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Distinguished Doctoral Scholarship, Zhejiang University&nbsp;&nbsp; 2018-2019</span> &nbsp;&nbsp;
</td></tr></tbody></table>



<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=6IxMkD9AxgkXmtT-CMTMolftstwgqiD1ExJBl3I8mPE&cl=ffffff&w=a"></script>

</div>
</div>



</body></html>
